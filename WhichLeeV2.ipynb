{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhichLee V2\n",
    "\n",
    "This is a sequel to last year's WhichLee, where this time the target output is 128 binary labels.\n",
    "\n",
    "Our starting point, for chuckles and giggles, is a flag image from [a writeup from last year](https://gist.github.com/duckness/39f8feab4cb8ef0db075f30a29547827#file-whichlee-md):\n",
    "![lastyear](whichlee_lastyear.png)\n",
    "\n",
    "Chucking this whole image (LHL plus buttons and text) into the website, we get:\n",
    "\n",
    "```Your hash is 14faea6f19ff82c31694c2e7d1fa1b17, you are not the right LEE!```\n",
    "\n",
    "A quick view source suggests that the desired hash is `14caca6f19fe8281d6d6eae7d1f81b11`, so we're actually not that far off! First let's make sure we can reproduce the same hash locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14faea6f19ff82c31694c2e7d1fa1b17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "\n",
    "#path = sys.argv[1]\n",
    "path = 'whichlee_lastyear.png'\n",
    "N_BITS= 128\n",
    "IMG_SZ = 32\n",
    "\n",
    "# stolen from https://towardsdatascience.com/implementing-yann-lecuns-lenet-5-in-pytorch-5e05a0911320\n",
    "class LeeNet5(nn.Module):\n",
    "    def __init__(self, n_bits):\n",
    "        super(LeeNet5, self).__init__()\n",
    "        self.n_bits = n_bits\n",
    "        self.feature_extractor = nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=n_bits, kernel_size=5, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=n_bits, out_features=n_bits),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=n_bits, out_features=n_bits),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        nBatch = x.shape[0]\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.reshape((nBatch,self.n_bits))\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def get_img_tensor(img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.Resize((IMG_SZ,IMG_SZ)),\n",
    "        transforms.ConvertImageDtype(torch.float)\n",
    "    ])\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img_tensor = transform(img).reshape((1,3,IMG_SZ,IMG_SZ))\n",
    "    return img_tensor\n",
    "\n",
    "def hash(model, img_tensor, hashing_matrix):\n",
    "    output = model(img_tensor)\n",
    "    y = torch.matmul(output, hashing_matrix).flatten()\n",
    "    Hx = nn.functional.threshold(y, 0, 0)\n",
    "    Hx = nn.functional.threshold(-Hx, -0.000000000000001, 1)\n",
    "    Hx = Hx.type(torch.IntTensor)\n",
    "    Hx = Hx.tolist()\n",
    "    return hex(int(\"\".join([str(x) for x in Hx]),2))[2:]\n",
    "\n",
    "\n",
    "model = LeeNet5(N_BITS)\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "model.eval()\n",
    "hashing_matrix = torch.load(\"hashing_matrix.pt\")\n",
    "\n",
    "img_tensor = get_img_tensor(path)\n",
    "print(hash(model, img_tensor, hashing_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, it's the same hash, so let's analyse the neural net quickly. Most of it is pretty standard, but at the end the output of the network is multiplied by a hashing matrix and flattened to a vector of length 128, then this is finally binarily classified depending on its sign. So the output is 128 binary labels, which can be printed as a hash.\n",
    "\n",
    "The solution here then, is to just tweak [4yn's classic solution to last year's WhichLee](https://github.com/4yn/slashbadctf/blob/master/sgctf21/which-lee/which-lee-solution.md). To summarise how it works, we basically start with an input and pass it through the model to infer its output. We evaluate the loss and backpropagate the gradients back through the network to get an input that more closely matches the desired output. This isn't an exact science of course, so we have to tweak various things like:\n",
    "- the initial output: tried various images, this one just happened to work\n",
    "- the loss metric: what worked at the end was an MSE Loss against a vector of 2s and -2s\n",
    "- the learning rate: value of 100 from the original writeup seems to work.\n",
    "\n",
    "Also, we may find a feasible input only to realise that it cannot be return-tripped from an image, so we will also output an image at every step just to check.\n",
    "\n",
    "Otherwise, that's basically it. Rinse and repeat until we get a feasible output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win on iteration 79\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "orig_img_tensor = img_tensor\n",
    "for i in range(9999):\n",
    "    \n",
    "    targetstr = '14caca6f19fe8281d6d6eae7d1f81b11'\n",
    "    targetarr = [-2*(-1)**int(i) for i in f'{int(targetstr, 16):0128b}']\n",
    "    target = torch.Tensor(targetarr)\n",
    "    \n",
    "    img_tensor.requires_grad = True\n",
    "    img_tensor.grad = None\n",
    "        \n",
    "    save_image(img_tensor, 'whichlee_test.png')\n",
    "    h = hash(model, get_img_tensor('whichlee_test.png'), hashing_matrix)\n",
    "    if h == targetstr:\n",
    "        print(f'Win on iteration {i}')\n",
    "        break\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    foo = torch.matmul(model(img_tensor), hashing_matrix).flatten()\n",
    "    output = loss(foo, target)\n",
    "    output.backward()\n",
    "\n",
    "    img_tensor = img_tensor - img_tensor.grad * 100\n",
    "    arr = img_tensor.detach().numpy()\n",
    "    img_tensor = torch.Tensor(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's the image in all its glory!\n",
    "![test](whichlee_test.png)\n",
    "\n",
    "To complete the circular reference, here's the flag image, which can maybe be used as in input to WhichLee V3 next year:\n",
    "![flag](whichlee_flag.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
